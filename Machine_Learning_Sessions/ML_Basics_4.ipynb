{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c3a3018",
   "metadata": {},
   "source": [
    "# ML Basics \n",
    "##### 15-April-2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa7da80",
   "metadata": {},
   "source": [
    "### Model Regularisation: How to reduce it's complexity so that they don't overfit\n",
    "    \n",
    "#### Weight Penalty\n",
    "To prevent overfitting, we modify our loss function\n",
    "L_reg = L + Lambda * R\n",
    "\n",
    "Here,\n",
    "\n",
    "L = Loss function (MSE , Log-loss etc)\n",
    "\n",
    "R = Regulariser\n",
    "\n",
    "Lambda = Regularisation Strength\n",
    "\n",
    "#### Example: L2 Penalty\n",
    "Here R = sum of squares of weights (Not including the bias)\n",
    "\n",
    "This prevents the weights from becoming too large. Thus reduces overfitting. It can be optimized using gradient descent since it is easily differentiable.\n",
    "\n",
    "\n",
    "#### Another example: L1 Penalty\n",
    "Here we take sum of absolute values of weights (Not including bias)\n",
    "- It cannot be optimized using simple Gradient descent methods. (Needs some advanced optimization techniques)\n",
    "Advantage: It leads to sparse solution. i.e the model can lead some of the weights to 0. Thus it depends only on some subset of features.\n",
    "\n",
    "#### Other regularisation methods:\n",
    " - Dimentionality reduction (Remove some features or apply principal component analasys to get good features)\n",
    " - Data augmentation (Distort flip rotate images etc)\n",
    " - Dropout (Will be discussed in coming sessions)\n",
    " - Early stopping\n",
    " - Collect more data (More data i.e Harder to overfit)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398bfe54",
   "metadata": {},
   "source": [
    "## Data Preprocessing \n",
    "\n",
    "#### What if one feature was in Km and the other in m?\n",
    "\n",
    "##### Feature scaling :\n",
    "Ex: Car distance travelled & Car length, Annual Income & Age etc\n",
    "    This is used to standardise the features. This makes sure that none of the features (Or input Data) dominates any other.\n",
    "    \n",
    "    \n",
    "\n",
    "##### Data Encoding:\n",
    "Say you have a data with 5 categories. Which subject the student scored the highest in. \n",
    "\n",
    "If we label them as \n",
    "` 1,2,3,4,5 ` The ML model will assume that the order is significant, it will assume the difference between each category is the same, wich is wrong. \n",
    "\n",
    "So in these cases we use one-hot encoding.\n",
    "``` \n",
    "    1 -> 1 0 0 0 0\n",
    "    2 -> 0 1 0 0 0\n",
    "    2 -> 0 0 1 0 0 \n",
    "```\n",
    "\n",
    "Only one of them is high depending on the category. This helps the model understand the problem better.\n",
    "    \n",
    "**This can be done by Standardisation:**\n",
    "X' = (X - mean(X)) / Standard_Deviation\n",
    "\n",
    "**and by Normalisation:**\n",
    "X' = (X - min(X))  / (max(X)-min(X))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0434816",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "Feature engineering is the pre-processing step of machine learning, which extracts features from raw data\n",
    "- **Feature Creation:**\n",
    "    Creating features involves creating new variables which will be most helpful for our model. \n",
    "    This can be adding or removing some features. \n",
    "    For example, creating a feature called the cost per sq. ft using total cost and area of a property can help find mistakes using domain knowledge as well as be a usefull feature.\n",
    "\n",
    "- **Transformations:** \n",
    "    Transforms features from one representation to another. Say rotating, zooming and cropping, distorting etc\n",
    "\n",
    "- **Feature Extraction:**\n",
    "    Extracting usefull features from the data.Without distorting the original relationships or significant information,\n",
    "    this compresses the amount of data into manageable quantities for algorithms to process.\n",
    "    \n",
    "### Exploratory Data Analysis : \n",
    "Exploratory data analysis (EDA) is a powerful and simple tool that can be used to improve your understanding of your data, by exploring its properties. \n",
    "The technique is often applied when the goal is to create new hypotheses or find patterns in the data. It’s often used on large amounts of qualitative or quantitative data that haven’t been analyzed before.\n",
    "\n",
    "This involves exploring and transforming your data to understand it well. Things like patterns and relations found will be useful while deciding and implementing your model as well as while deciding the features.\n",
    "Plotting the data in various ways also helps visualize and understand the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4104f0b2",
   "metadata": {},
   "source": [
    "# Multi Layered Perceptron (MLP)\n",
    "\n",
    "Note: In the previous topics, I've given a surface level knowledge of each topic. You must read up on them to learn more details. Ex:momentum in gradient descent etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73b209d8",
   "metadata": {},
   "source": [
    "*Reference: Slides borrowed from Introduction to Deep Learning Course by HSE University on Coursera.*\n",
    "\n",
    "### Linear Binary Classification:\n",
    "   <img src=\"Images/LinearBinaryClassification.jpg\" alt=\"LinearBinaryClassification\" width=\"400\"/>\n",
    "   \n",
    "   Here, we have two features x1 and x2, two weights w1 and w2 and a bias w0.\n",
    "   Based on the linear equation, we have a predicion if it is positive and the other prediction if it is negative. Thus this is used for binary classification.\n",
    "   \n",
    "   Rather that using a sigmoid function, if we use a sigmoid function. We get a logistic regression.\n",
    "  ### Logistic Regression:\n",
    "  \n",
    "  <img src=\"Images/LogisticRegression.jpg\" alt=\"LogisticRegression\" width=\"600\"/>\n",
    "  \n",
    "  We know that the value of the output of the linear function is proportional to the distance from the line. \n",
    "  Passing it through the sigmoid function converts this distance into a measure of confidence.i.e The closer to 1 or 0 it is, the more likely the classification is correct.\n",
    "  ~0.5 would mean it is near the border of the classes and we are not that confident that it is classified correctly.\n",
    "  \n",
    "  Now say we have this triangle problem\n",
    "  <img src=\"Images/TriangleProblem.jpg\" alt=\"TriangleProblem\" width=\"400\"/>\n",
    "  \n",
    "  It is obvious we cannot classify it perfectly into 2 parts using logistic regression.\n",
    "  But, say we found 1 line like this:\n",
    "  <img src=\"Images/Line1.jpg\" alt=\"TriangleProblem\" width=\"600\"/>\n",
    "  \n",
    "  Similarly, with three lines\n",
    "  <img src=\"Images/NewFeatures.jpg\" alt=\"TriangleProblem\" width=\"600\"/>\n",
    "  \n",
    "  So we can now train a linear model on the three calculated features:\n",
    "  <img src=\"Images/ComputationGraph.jpg\" alt=\"TriangleProblem\" width=\"600\"/>\n",
    "\n",
    "  This is what we call a Multi Layered Perceptron (MLP):\n",
    "  <img src=\"Images/MLP.jpg\" alt=\"TriangleProblem\" width=\"600\"/>\n",
    "\n",
    "  It has 3 types of layers namely:\n",
    "  - The Input layer\n",
    "  - The Output Layer\n",
    "  - The hidden layers\n",
    "  \n",
    "  Each node is called a neuron, and the sigmoid function used after each linear combination is called an activation function.\n",
    "  \n",
    "  Thus MLP comes under a group of Machine learning models called **Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcba382",
   "metadata": {},
   "source": [
    "We will discuss NN further in the next session"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
