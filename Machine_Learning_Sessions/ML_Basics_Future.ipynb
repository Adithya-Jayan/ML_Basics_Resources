{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a46d8c09",
   "metadata": {},
   "source": [
    "# Material in progress for further sessions\n",
    "Adithya Jayan\n",
    "\n",
    "Code borrowed from: Coursera - Introduction to deep learning - HSE university"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e8604a",
   "metadata": {},
   "source": [
    "### Visualize maximum stimuli (Not necessary - just for visualization of learnt features)\n",
    "\n",
    "We want to find input images that provide maximum activations for particular layers of our network. \n",
    "\n",
    "We will find those maximum stimuli via gradient ascent in image space.\n",
    "\n",
    "For that task we load our model weights, calculate the layer output gradient with respect to image input and shift input image in that direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a68780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "s = tf.keras.backend.clear_session()  # clear default graph\n",
    "\n",
    "last_finished_epoch = 3\n",
    "\n",
    "K.set_learning_phase(0)  # disable dropout\n",
    "model = load_model(model_filename.format(epoch = last_finished_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308e5615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all weights we have\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6edbe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_maximum_stimuli(layer_name, is_conv, filter_index, model, iterations=20, step=1., verbose=True):\n",
    "    \n",
    "    def image_values_to_rgb(x):\n",
    "        # normalize x: center on 0 (np.mean(x_train2)), ensure std is 0.25 (np.std(x_train2))\n",
    "        # so that it looks like a normalized image input for our network\n",
    "        x = x-np.mean(x)\n",
    "        x=x/np.sqrt(4*np.std(x))\n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "        # do reverse normalization to RGB values: x = (x_norm + 0.5) * 255\n",
    "        x = (x+0.5) * 255   ### YOUR CODE HERE\n",
    "    \n",
    "        # clip values to [0, 255] and convert to bytes\n",
    "        x = np.clip(x, 0, 255).astype('uint8')\n",
    "        return x\n",
    "\n",
    "    # this is the placeholder for the input image\n",
    "    input_img = model.input\n",
    "    img_width, img_height = input_img.shape.as_list()[1:3]\n",
    "    \n",
    "    # find the layer output by name\n",
    "    layer_output = list(filter(lambda x: x.name == layer_name, model.layers))[0].output\n",
    "\n",
    "    # we build a loss function that maximizes the activation\n",
    "    # of the filter_index filter of the layer considered\n",
    "    if is_conv:\n",
    "        # mean over feature map values for convolutional layer\n",
    "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "    else:\n",
    "        loss = K.mean(layer_output[:, filter_index])\n",
    "\n",
    "    # we compute the gradient of the loss wrt input image\n",
    "    grads = K.gradients(loss, input_img)[0]  # [0] because of the batch dimension!\n",
    "\n",
    "    # normalization trick: we normalize the gradient\n",
    "    grads = grads / (K.sqrt(K.sum(K.square(grads))) + 1e-10)\n",
    "\n",
    "    # this function returns the loss and grads given the input picture\n",
    "    iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "    # we start from a gray image with some random noise\n",
    "    input_img_data = np.random.random((1, img_width, img_height, 3))\n",
    "    input_img_data = (input_img_data - 0.5) * (0.1 if is_conv else 0.001)\n",
    "\n",
    "    # we run gradient ascent\n",
    "    for i in range(iterations):\n",
    "        loss_value, grads_value = iterate([input_img_data])\n",
    "        input_img_data += grads_value * step\n",
    "        if verbose:\n",
    "            print('Current loss value:', loss_value)\n",
    "\n",
    "    # decode the resulting input image\n",
    "    img = image_values_to_rgb(input_img_data[0])\n",
    "    \n",
    "    return img, loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820f9ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample maximum stimuli\n",
    "def plot_filters_stimuli(layer_name, is_conv, model, iterations=20, step=1., verbose=False):\n",
    "    cols = 8\n",
    "    rows = 2\n",
    "    filter_index = 0\n",
    "    max_filter_index = list(filter(lambda x: x.name == layer_name, model.layers))[0].output.shape.as_list()[-1] - 1\n",
    "    fig = plt.figure(figsize=(2 * cols - 1, 3 * rows - 1))\n",
    "    for i in range(cols):\n",
    "        for j in range(rows):\n",
    "            if filter_index <= max_filter_index:\n",
    "                ax = fig.add_subplot(rows, cols, i * rows + j + 1)\n",
    "                ax.grid('off')\n",
    "                ax.axis('off')\n",
    "                loss = -1e20\n",
    "                while loss < 0 and filter_index <= max_filter_index:\n",
    "                    stimuli, loss = find_maximum_stimuli(layer_name, is_conv, filter_index, model,\n",
    "                                                         iterations, step, verbose=verbose)\n",
    "                    filter_index += 1\n",
    "                if loss > 0:\n",
    "                    ax.imshow(stimuli)\n",
    "                    ax.set_title(\"Filter #{}\".format(filter_index))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2376d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum stimuli for convolutional neurons\n",
    "conv_activation_layers = []\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, LeakyReLU):\n",
    "        prev_layer = layer.inbound_nodes[0].inbound_layers[0]\n",
    "        if isinstance(prev_layer, Conv2D):\n",
    "            conv_activation_layers.append(layer)\n",
    "\n",
    "for layer in conv_activation_layers:\n",
    "    print(layer.name)\n",
    "    plot_filters_stimuli(layer_name=layer.name, is_conv=True, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc3035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum stimuli for last dense layer\n",
    "last_dense_layer = list(filter(lambda x: isinstance(x, Dense), model.layers))[-1]\n",
    "plot_filters_stimuli(layer_name=last_dense_layer.name, is_conv=False, \n",
    "                     iterations=200, step=0.1, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d471fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaad609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417df8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403a63d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63e0743b",
   "metadata": {},
   "source": [
    "Entropy, Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d132153e",
   "metadata": {},
   "source": [
    "#### Deciding your model\n",
    "\n",
    "Deciding your model might not be straightforward. It depends on various factors like the type of data, the output needed, what we're trying to achive, how fast it needs to be and a lot more.\n",
    "\n",
    "<img src=\"Images/Misunderstanding.jpg\" alt=\"Overfit Model\" width=\"500\"/>\n",
    "\n",
    "<img src=\"Images/Info.jpg\" alt=\"Overfit Model\" width=\"300\"/>\n",
    "\n",
    "ML is like a language, you need to convey what you need through the model\n",
    "- Ex: say you want to predict car price\n",
    "<img src=\"Images/Car1.jpg\" alt=\"Overfit Model\" width=\"500\"/>\n",
    "- Naive way would be to do this\n",
    "<img src=\"Images/Car2.jpg\" alt=\"Overfit Model\" width=\"500\"/>\n",
    "- With some more knowledge, we would know that the images are low level and hence would need some more proccesing compared to high level data.\n",
    "<img src=\"Images/Car3.jpg\" alt=\"Overfit Model\" width=\"500\"/>\n",
    "- Say we wanted to reduce importance of the images on final output\n",
    "<img src=\"Images/Car4.jpg\" alt=\"Overfit Model\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e19052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0923d7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc9a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7961c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a0d7da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
