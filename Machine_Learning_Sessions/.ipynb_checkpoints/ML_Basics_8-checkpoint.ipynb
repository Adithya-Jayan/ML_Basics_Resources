{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64db7d1f",
   "metadata": {},
   "source": [
    "# Session 8\n",
    "#### 24-April-2022\n",
    "Note: Some images and materials have been borrowed from the HSE: Introduction to deep learning course - from coursera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d31584",
   "metadata": {},
   "source": [
    "## Transfer learning:\n",
    "\n",
    "In general, the first few layers \"extract\" Useful features from the data. These are converted to higher level features as we go down each layer.\n",
    "The actual \"Predictions\" are done using these higher level features by the last few dense layers. Thus we can change what we want to predict just by changing those few layers!\n",
    "\n",
    "<img src=\"Images/TransferLearning.jpg\" alt=\"Transfer Learning\" width=\"500\"/>\n",
    "\n",
    "- Reduces the amount of data and time required to train the model\n",
    "- Can be used as long as the output is somewhat similar to the other model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3f8d6c",
   "metadata": {},
   "source": [
    "Say I want to make a model that predicts human emotions. I already have a model-  Imagenet (A model used for object recognition).\n",
    "\n",
    "Since Imagenet was trained on objects and not faces, it cannot help with recognising faces. But lower level features it generates can still be useful!\n",
    "\n",
    "\n",
    "<img src=\"Images/ImageNetFeatures.jpg\" alt=\"FeatureLevels\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde031a",
   "metadata": {},
   "source": [
    "Again, we can try and reuse the learnt features in some of the deeper levels too. This can be done by initializing with the trained weights instead of random initialization.\n",
    "We can use a small learning rate on these weights. This is called **Fine Tuning**\n",
    "\n",
    "\n",
    "<img src=\"Images/FineTuning.jpg\" alt=\"Transfer Learning\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851632cb",
   "metadata": {},
   "source": [
    "This is often used for very large models which require huge datasets and months of time to train. Many such pre-trainined models\n",
    "are available (VGG, Inception,Resnet, LSTM) which can be used for transfer learning/fine tuning for use in similar applications.\n",
    "\n",
    "InceptionV3 Archetecture:\n",
    "<img src=\"Images/InceptionV3.jpg\" alt=\"Inception\" width=\"900\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52138378",
   "metadata": {},
   "source": [
    "Takeaways?\n",
    "<img src=\"Images/TLTakeaways.jpg\" alt=\"Transfer Learning\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487f506c",
   "metadata": {},
   "source": [
    "### AutoEncoders\n",
    "What is it?\n",
    "\n",
    "It is a model designed to reduce the dimentionality of your data.\n",
    "\n",
    "<img src=\"Images/Autoencoder.png\" alt=\"Transfer Learning\" width=\"800\"/>\n",
    "Here a \"Bottle neck\" is used for reducing the dimentionality and forcing the model to generalize the info present in the images.\n",
    "\n",
    "This latent space representation can then be used as inputs for other models (Like XGBoost) or as a compressed representation of the original data.\n",
    "\n",
    "<img src=\"Images/Autoencoder2.jpg\" alt=\"Transfer Learning\" width=\"500\"/>\n",
    "\n",
    "The Autoencoder has two parts\n",
    "- The encoder\n",
    "- The decoder\n",
    "\n",
    "One does the reverse of the other. i.e we use upsampling instead of downsampling etc\n",
    "\n",
    "<img src=\"Images/InverseConv.jpg\" alt=\"Transfer Learning\" width=\"500\"/>\n",
    "\n",
    "We try to obtain the best possible deconstruction of the data while training Autoencoders. Since we do not have labelled data for training here,\n",
    "this categorizes as a form of **unsupervised learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93008697",
   "metadata": {},
   "source": [
    "#AutoEncoder Applications\n",
    "- Principle component analysis (PCA)\n",
    "<img src=\"Images/PCA.jpg\" alt=\"PCA\" width=\"500\"/>\n",
    "\n",
    "- Say we distort the input with added noise. And train the autoencoder with clean images. Then the model will learn to essentially perform denoising\n",
    "<img src=\"Images/Denoising.jpg\" alt=\"Denoising\" width=\"500\"/>\n",
    "\n",
    "- We could also use it for pretraining models.\n",
    "<img src=\"Images/Pretraining1.jpg\" alt=\"Pretraining1\" width=\"500\"/>\n",
    "\n",
    "<img src=\"Images/Pretraining2.jpg\" alt=\"Pretraining2\" width=\"500\"/>\n",
    "\n",
    "- Other uses include morphing between images\n",
    "Images are represented with lower dimensions in the encoded representation. Thus changing the encoded values slightly will give a gradual transition between outputs.\n",
    "\n",
    "Say for a face dataset. Taking the difference between an older and younger face (This is now something like an \"add age\" vector), and adding it to a new face - might give an older version of that new face!\n",
    "<img src=\"Images/ImageMorphing.jpg\" alt=\"ImageMorphing\" width=\"500\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453920d4",
   "metadata": {},
   "source": [
    "Supervised learning requires a large amout of labelled data, which is hard to obtain.\n",
    "Luckily, we can use transfer learning to halp us with that - but in general, it can only be used for popular problems.\n",
    "\n",
    "Unsupervised learning on the other-hand does not require any labelled data. Thus is is a lot easier to obtain data.\n",
    "But, it might learn features that are irrelevant to our problem. (Ex: background sky colour for object classification)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
